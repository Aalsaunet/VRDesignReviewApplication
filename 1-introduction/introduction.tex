\section{Background}            
The field of virtual- and augmented reality technologies has seen an exciting development in recent years, 
with the first release of commercial virtual reality devices, such as Oculus Rift and HTC Vive, taking place in 2016.



Although these devices' initial target market was the games- and entertainment industry, 
the applications of these devices in other domains have already exceeded the expectations of many~\citep{VRS2016}. 
Examples of such domains include the military, the educational system, the healthcare industry, the construction businesses and the telecommunications industry~\citep{VRS2016}. 

\section{Virtual Technology}
Virtual reality can be defined as a realistic and immersive simulation of a three-dimensional 360 degree environment, 
created using interactive software and hardware, and experienced or controlled by movement of the body \citep{VRS2016}.
This virtual environment is perceived through a virtual reality headset, which are head mounted goggles that displays a screen in front of each eye, and contain sensors to 
record the users head movements. A person using a virtual reality headset should thus able to "look around" this virtual world by turning his or her head.




%These devices enable the user to be completely emerged in 3D virtual worlds by having one high resolution lens per eye, 
%covering the user's whole field of vision. 


As virtual reality technology enables users to visually perceive virtual worlds in a new way, 
human-computer interaction (HCI) is also a highly relevant topic. 
This field has in many ways seen a resurgence as virtual technology gives new possibilities, but also set new constrains. 
One of these constraints is limiting the user's field of vision exclusively to that projected by the lenses, 
which may make interaction with traditional input devices, such as mouse and keyboard, more challenging. 
Because of this, alternate methods of interacting with the computer is a relevant topic. 
One of these methods is the use of gestures, 
which have long been considered an interaction technique that can potentially deliver more natural, 
creative and intuitive methods for communicating with our computers~\citep{Rautaray2015}. 
To enable the use of gestures as a viable input method to a computer, responsive and reliable gesture recognition techniques are needed.  

\section{Gesture Recognition Technology}
A gesture can be defined as a physical movement of the hands, arms, face and body with the intent to convey information or meaning~\citep{Mitra2007}, 
Even though the use of keyboard and mouse is a prominent interaction method, there are situations in which
these devices are impractical for human-computer interaction (HCI). This is particularly the case for interaction with 3D objects~\citep{Rautaray2015}. 

To be able to convey semantically meaningful commands through the use of gestures one must rely on a gesture recognition system, 
which is responsible for capturing and interpreting gestures from the user and, if applicable, carry out the desired action. 
Often this process is seen as a sum of three fundamental phases: Detection, tracking and recognition~\citep{Rautaray2015}.

\subsection{Detection}
The first step in a typical gesture recognition system is to detect the relevant parts of the captured image and segment them from the rest. 
This segmentation is crucial because it isolates the relevant parts of the image from the background to ensure that only the relevant part is processed by the subsequent 
tracking and recognition stages~\citep{Cote2006}. 
A gesture recognition system will typically be interested in hand gestures, head- and arm movements and body poses, and thus only these factors should be observed by the system.

\subsection{Tracking}
The second step in a gesture recognition system is to track the movements of the relevant segments of the frames, e.g.~the hands. 
Tracking can be described as the frame-to-frame correspondence of the segmented hand regions and aims to understand the observed hand movements. 
This is often a difficult task as hands can move very fast and their appearance can change vastly within a few frames, 
especially when light condition is a big factor~\citep{Wang2010}. 
One additional note is that if the detection method used is fast enough to operate at image acquisition frame rate, it can also be used for tracking~\citep{Rautaray2015}.   

\subsection{Recognition}
The last step of a gesture recognition system is to detect when a gesture occurs. 
This often implies checking against a predefined set of gestures, each entailing a specific action. 
To detect static gestures (i.e postures involving no movement) a general classifier or template-matcher can be used, 
but with dynamic gestures (which involves movement) other methods, which keep the temporal aspect, such as a Hidden Markov Model (HMM), are often required~\citep{Benton1995}. 
The recognition technology often makes uses of several methods from the field of machine learning, including supervised, unsupervised and reinforced learning.

\section{Problem definition}
This thesis will explore the possibilities of utilizing virtual reality- and gesture recognition technology in combination 
during the design review process of the major international classification company DNV GL. 
This will be accomplished by implementing a design review application that makes use of both these technologies for several design review tasks (see section 2.1), 
and by having users evaluate the effectiveness of these techniques. 
This essay will summarize DNV GL's design review process, list the application requirements, 
discuss some of the developments within the field of gesture recognition technology and briefly review the Leap Motion Controller.

\section{Limitations}

\section{Outline}