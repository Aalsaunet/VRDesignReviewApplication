To evaluate the application's ability to meet user requirements, two rounds of user testing seasons were conducted at the DNV GL headquarters in HÃ¸vik, Norway.
The first of these season were held the 24th March 2017 and involved one test person, while the second round were held at the 30th March 2017 and involved two persons. 
The users were brought in individually and asked to take a seated position at an ordinary work stations with a mouse, keyboard and display, 
in addition to a leap motion controller positioned at the desk between the keyboard and the user. A HTC Vive head mount was also present for use during the experimentation phase. 

The computer used for the testing had the following specifications (hardware and software):
\begin{itemize}
    \item An Intel i7 as processor.
    \item 8 GB of RAM.
    \item A Nvidia Geforce GTX 1080 graphics card.
    \item A Windows 10 64-bit operating system (build 14393).
    \item Unity 5.5.2
    \item Leap Motion Control Panel version 3.2.0+45899
    \item Steam VR runtime (for use with the HTC Vive head mount)
\end{itemize}

After the user was seated the test phases were conducted in the following order (including an estimated of alloted time):

\begin{enumerate}
    \item  5 minutes of introduction. The users were informed about the purpose of the application, some of its long term goals and its limitations.
    \item 10 minutes of demonstration. The users were shown each of the possible actions and the different gestures available to them.
    \item 15 minutes of instructions. The users followed a series of instructions and oral explanations to teach them to use the program.
    \item 20 minutes of experimentation. The users were asked to use the program freely without any instructions.  
    \item 10 minutes of questions. The users were interviewed with a series of questions related to the application and their experience using it.   
\end{enumerate}

With the exception of the experimentation phase, all the steps above were conducted without the use of a VR head mount. In the experimentation phase
the users were asked to divide their time equally between using the application in "desktop mode" (i.e using an regular display without a VR head mount) and
"VR mode" (i.e using a VR head mount). 

\section{The instructions}
The users were asked to perform the following tasks:

\begin{enumerate}
    \item The pinch gesture is performed by pushing the thumb and index finger together, while keeping the palm directed against the table surface. 
          Move the hand which holding the pinch gesture to rotate the camera along the X and Y axis.
    
    \item The X gesture is performed by holding your hand straight with all fingers extended, pointing towards the screen and the palm facing downward towards the table surface. 
          Lift and lower your hand to change move the camera along the Y axis. 
    
    \item The Y gesture is performed by holding your hand straight with all fingers extended, pointing towards the screen and the palm facing to the side, 
          perpendicular to the table surface. 
          Move it from side to side to move the camera along the X axis.
    
    \item The Z gesture is performed by holding your hand curled up into a fist with no finger extended, pointing towards the screen and the palm facing downward towards the table surface. 
          Move your fist closer or further from the screen to move the camera along the Z axis. 
    
    \item Maneuver from your current position around one of the pipes present in the 3D model and back to your original position, using one or both hands. 
    
    \item Hold your left hand straight and rotate it so the palm if facing towards you. A menu shaped like a fan should appear and follow the movements of your left hand as long as
          this gesture is held. Use the index finger of the right hand to select "Toggle Options" and then "Combine XYZ Gestures". 
          To select a button hold the tip of the right index finger close enough (in terms of X, Y and Z axis) to the button for it to gradually highlight. 
          When "Combine XYZ Gestures" has been selected the X, Y and Z gestures are combined/replaced by a combined XYZ gesture, which is performed the same way as the Y gesture
          (hand straight and palm down). When now performing and holding this gesture the user can move along the X, Y, and Z axis in the virtual space by moving the hand 
          correspondingly in the physical space.

    \item Maneuver as in instruction \#5, but this time by using in the combined XYZ gesture. After the user has completed this s/he might switch back to the other gesture scheme
          by bringing up the menu and select "Toggle Option" and "Distinguish XYZ Gestures", or keep the the combined XYZ gesture.

    \item By utilizing the gestures introduced thus far, move the camera so the cursor/crosshair in the middle of the screen is positioned over a nearby object. 
          Perform a pointing gesture by only having the index finger extended and point at the screen (away from you). If this is done correctly a blue sphere should occur, which
          is called an "Annotation Sphere". This is in short a unit of information related to the position it is attached to. Create two more Annotation Spheres by moving the 
          cursor/crosshair over other nearby surfaces and point. 
    
    \item Now annotate/mark an entire object or surface by pointing two fingers ("double pointing") instead of one. These two fingers should ideally be held in a bit of an angle, like a scissor. 
          When done correctly the entire surface or object the cursor/crosshair is indicating should be colored in a similar blueish color as the annotation spheres. 

    \item Now place the cursor/crosshair over an annotation sphere or an annotated object and either point (if an annotation sphere is selected) or double point (if an annotated
          object is selected). When done correctly a form containing a text field, a virtual keyboard and some buttons should be displayed. 

    \item Write "DNV GL" in the text field by utilizing the virtual keyboard. After this click on one of the colored buttons to set a color on there annotation (used to indicate a 
          priority), and click submit to save the changes to the annotation. 

    \item Open the same annotation again by and delete it by pressing the delete button.     
          % When "origin" has been selected the camera should appear in the same position
          % and with the same rotation as when this session started.
\end{enumerate}

\section{The questions}
At the end of the individual test session the users were asked the following questions:

\begin{enumerate}
      \item Did you prefer to have distinct gestures for movement along the X, Y or Z axis or did you prefer having it combined in a single gesture?
      \item How effective and responsive did you find:
      \begin{enumerate}
            \item The pinch gesture?
            \item The X gesture?
            \item The Y gesture?
            \item The Z gesture?
            \item The combined gesture?
            \item The point gesture?
            \item The double point gesture?
      \end{enumerate}
      \item How easy to use was the menu?
      \item How difficult was it to place the cursor/crosshair where you wanted?
      \item How difficult or impractical was it to use the annotation form?
      \item How was using the application with a virtual reality head mount different from using it in "desktop mode"? 
            Which one did you prefer?
\end{enumerate}

% Write about how the participants responded.

