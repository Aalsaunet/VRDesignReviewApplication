To evaluate the application's ability to meet user requirements, two rounds of user testing seasons were conducted at the DNV GL headquarters in Høvik, Norway.
The first of these season were held the 24th March 2017 and involved one test person, while the second round were held at the 30th March 2017 and involved two persons. 
The participants were brought in individually and asked to take a seated position at an ordinary work stations with a mouse, keyboard and display, 
in addition to a leap motion controller positioned at the desk between the keyboard and the user. A HTC Vive head mount was also present for use during the experimentation phase. 

The computer used for the testing had the following specifications (hardware and software):
\begin{itemize}
    \item An Intel i7 as processor.
    \item 8 GB of RAM.
    \item A Nvidia Geforce GTX 1080 graphics card.
    \item A Windows 10 64-bit operating system (build 14393).
    \item Unity 5.5.2
    \item Leap Motion Control Panel version 3.2.0+45899
    \item Steam VR runtime (for use with the HTC Vive head mount)
\end{itemize}

After the participant was seated the test phases were conducted in the following order (including an estimated of alloted time):

\begin{enumerate}
    \item  5 minutes of introduction. The users were informed about the purpose of the application, some of its long term goals and its limitations.
    \item 10 minutes of demonstration. The users were shown each of the possible actions and the different gestures available to them.
    \item 15 minutes of instructions. The users followed a series of instructions and oral explanations to teach them to use the program.
    \item 20 minutes of experimentation. The users were asked to use the program freely without any instructions.  
    \item 10 minutes of questions. The users were interviewed with a series of questions related to the application and their experience using it.   
\end{enumerate}

With the exception of the experimentation phase, all the steps above were conducted without the use of a VR head mount. In the experimentation phase
the participants were asked to divide their time equally between using the application in "desktop mode" (i.e using an regular display without a VR head mount) and
"VR mode" (i.e using a VR head mount). 

\section{The Instructions}
The participants were asked to perform the following tasks:

\begin{enumerate}
    \item The pinch gesture is performed by pushing the thumb and index finger together, while keeping the palm directed against the table surface. 
          Move the hand which holding the pinch gesture to rotate the camera along the X and Y axis.
    
    \item The X gesture is performed by holding your hand straight with all fingers extended, pointing towards the screen and the palm facing downward towards the table surface. 
          Lift and lower your hand to change move the camera along the Y axis. 
    
    \item The Y gesture is performed by holding your hand straight with all fingers extended, pointing towards the screen and the palm facing to the side, 
          perpendicular to the table surface. 
          Move it from side to side to move the camera along the X axis.
    
    \item The Z gesture is performed by holding your hand curled up into a fist with no finger extended, pointing towards the screen and the palm facing downward towards the table surface. 
          Move your fist closer or further from the screen to move the camera along the Z axis. 
    
    \item Maneuver from your current position around one of the pipes present in the 3D model and back to your original position, using one or both hands. 
    
    \item Hold your left hand straight and rotate it so the palm if facing towards you. A menu shaped like a fan should appear and follow the movements of your left hand as long as
          this gesture is held. Use the index finger of the right hand to select "Toggle Options" and then "Combine XYZ Gestures". 
          To select a button hold the tip of the right index finger close enough (in terms of X, Y and Z axis) to the button for it to gradually highlight. 
          When "Combine XYZ Gestures" has been selected the X, Y and Z gestures are combined/replaced by a combined XYZ gesture, which is performed the same way as the Y gesture
          (hand straight and palm down). When now performing and holding this gesture the user can move along the X, Y, and Z axis in the virtual space by moving the hand 
          correspondingly in the physical space.

    \item Maneuver as in instruction \#5, but this time by using in the combined XYZ gesture. After the user has completed this s/he might switch back to the other gesture scheme
          by bringing up the menu and select "Toggle Option" and "Distinguish XYZ Gestures", or keep the the combined XYZ gesture.

    \item By utilizing the gestures introduced thus far, move the camera so the cursor/crosshair in the middle of the screen is positioned over a nearby object. 
          Perform a pointing gesture by only having the index finger extended and point at the screen (away from you). If this is done correctly a blue sphere should occur, which
          is called an "Annotation Sphere". This is in short a unit of information related to the position it is attached to. Create two more Annotation Spheres by moving the 
          cursor/crosshair over other nearby surfaces and point. 
    
    \item Now annotate/mark an entire object or surface by pointing two fingers ("double pointing") instead of one. These two fingers should ideally be held in a bit of an angle, like a scissor. 
          When done correctly the entire surface or object the cursor/crosshair is indicating should be colored in a similar blueish color as the annotation spheres. 

    \item Now place the cursor/crosshair over an annotation sphere or an annotated object and either point (if an annotation sphere is selected) or double point (if an annotated
          object is selected). When done correctly a form containing a text field, a virtual keyboard and some buttons should be displayed. 

    \item Write "DNV GL" in the text field by utilizing the virtual keyboard. After this click on one of the colored buttons to set a color on there annotation (used to indicate a 
          priority), and click submit to save the changes to the annotation. 

    \item Open the same annotation again by and delete it by pressing the delete button.     
          % When "origin" has been selected the camera should appear in the same position
          % and with the same rotation as when this session started.
\end{enumerate}

\section{The Questions}
At the end of the individual test session the users were asked the following questions:

\begin{enumerate}
      \item Did you prefer to have distinct gestures for movement along the X, Y or Z axis or did you prefer having it combined in a single gesture?
      \item How effective and responsive did you find:
      \begin{enumerate}
            \item The pinch gesture?
            \item The X gesture?
            \item The Y gesture?
            \item The Z gesture?
            \item The combined gesture?
            \item The point gesture?
            \item The double point gesture?
      \end{enumerate}
      \item How easy to use was the menu?
      \item How difficult or impractical was it to use the annotation form?
      \item How difficult was it to place the cursor/crosshair where you wanted?     
      \item How was using the application with a virtual reality head mount different from using it in "desktop mode"? 
            Which one did you prefer?
      \item Did or do you feel any symptoms of motion or virtual reality sickness after using the application in virtual reality mode?
\end{enumerate}

\section{Responses}
On question no. 1 two of the participants responded that they preferred having distinct gestures for movement along the x-, y- and z-axis, because 
it gives more precision and it's easier to avoid accidental movement, while the remaining participant preferred the combined gesture scheme as 
s/he found it more intuitive to use. Combined gestures were also favored by this participant because s/he felt that it was easier to perform the 
required tasks with only one hand, and s/he could thus switch between using the left and the right hand to combat fatigue (e.g.~"gorilla arm syndrome").  

When asking the participants about their impressions about the different gestures the answers were more unanimous in some aspect
as all participants preferred the fist gesture the least and the pinch gesture the most. As we will see in the next section there might be an inverse relationship between the two.
All the participants also reported that they didn't use the palm-down (y-axis movement) and palm-side (x-axis movement) that much and instead preferred to rotate to the 
direction they wanted to move and use the fist gesture (z-axis movement) to go there. All the participants also preferred the single-point gesture over the double-point gesture as 
the latter often was mistaken for the former. The participants also found the initial 25 degree on and off angle of the single- and double point detectors 
(see section~\vref{sec:singlepoint_detector}) to be to "generous", resulting in several annotation being placed by mistake. 
As a result of this the participants also were allowed to try with a stricter 15 degree on/off-angle, which all preferred (and thus is the application default).

The participants all seemed pretty satisfied with the menu, as they felt it was straight-forward to use. One participant felt that is was a little "too slow", by which
s/he referred to how the user of the menu is required to hold a button for about 1-2 second before a click is registered (to avoid accidental clicks).
Another participant also felt that is was to hard to read the menu text (i.e the labels), as he it was to small and pixelated. Both of these issues 
can quickly be adjusted in the implementation, by e.g~increasing the font size and applying anti-aliasing to the text, but this wasn't done during the testing.
With regard to using the annotation form the participants had similar remarks as to the menu. 

When asking the participant about the differences between using the application with or without a virtual reality HMD, and what they prefer, they unanimously responded
that they preferred to use the application in virtual reality mode. One of the reasons for this was that it was simply easier to position the crosshair/cursor at the 
desired location (i.e to aim) with the assistance of head movements. Although the participants felt that aiming with the crosshair/cursor was easiest in VR mode, they still 
didn't find it troublesome to do so in desktop mode. Another reason the participants preferred using the application with a VR HMD was because of the 
better depth vision they felt it provided. This was, according to one participant, especially beneficial with regards to using the menu and annotation form, as it 
was easier to click the buttons (i.e easier to see the relation between the hand models and the buttons). The added depth information was, according to another participant, 
also especially useful to understand the model better. 

The participants were also asked if they felt any symptoms of motion or virtual reality sickness after using the application. 
One participant responded that s/he felt a little dizzy after using the application and that it felt like this was especially caused by unintentional movement 
(e.g.~When intentionally performing a movement gesture). The other two participant felt no kind of symptoms or fatigue after using the application.

\section{Observations}
As mentioned in the previous section, the fist gesture was the least preferred gesture.
This appeared to be because of two primary reason: Detector conflict and fatigue.
When the participants attempted to do a fist gesture the gesture recognition system would often mistake it for a pinch gesture,
possibly because the distance between the tip of the thumb and index finger is relatively small when forming a fist. As the Leap Motion base pinch detector only defines a pinch
as a small enough thump--tip-to-index-tip distance, additional detectors was added to the pinch gesture to avoid this detector conflict. 
Because of this, there are two composite pinch
detectors per hand in the implementation, one "slack" version only using the base pinch detector and one "strict" version using a combination of the base
pinch detector and the base finger extended detector. The strict version requires a pinch gesture to be performed while having at least two fingers extended at the same time. 
This makes it more distinguishable from the fist gesture, but also make it a bit harder to use (i.e more false negatives).
% Another reason why the fist gesture is the least preferred among the participants might also be because of ergonomic comfort. % Muligens noe annet enn "ergonomic comfort"?
% While in a seated position, many of the other gestures can be used with the upper part of the arm (i.e above the elbow) relatively stationary, but when
% using the fist gesture the user is often required to extend 


During the test session several of the participant also had interesting suggestions for the application.
One participant said he appreciated not having collision on the \texttt{MasterController} in the application, thus being able to move through objects, 
but that he would prefer if the user "bumped into objects" before going through them. S/he thus felt that some sort of collision, while still having the ability to move
through objects, would be helpful when positioning. 
Another user, upon discussing the fist-pinch detector conflict, suggested to use a new gesture to rotate the camera. 
More specifically, the participant suggested a fist gesture with the thumb extended to rotate and the regular fist gesture for Z-axis movement (thus using a gesture
scheme without the pinch gesture).


% Collision 

%As the test were run in the Unity editor, some tweaking could be done during the testing after request from the users.
%, as it was often mistaken for a pinch gesture (this will be discussed more below) . 

\section{Lessons Learned}

Although the sample size of these test was too small to draw definitive conclusions, they do point towards some hypothesizes and theories. 
Generally it feels like users of a gesture recognition system preferred false negatives over false positives, i.e they would rather have it occasionally
happen that a gesture they attempt aren't recognized than that the system detects a gesture which isn't being performed.




% \subsection{User proposals}

% Write about how the participants responded.

% Carsten:
% Nice! Obviously, the results of the study go here.
% 
% This must be followed by lessons learned. Supposedly, the lessons learned stay in this chapter, unless they are
% very fundamental and there are a lot of them, in which case you’d make an additional chapter.

